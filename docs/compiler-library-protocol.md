# CPrime Compiler-Library Protocol

## Overview

The CPrime coroutine system introduces a revolutionary **compiler-library cooperation protocol** that bridges static analysis with runtime optimization. Unlike traditional approaches where the compiler or runtime operates independently, CPrime's design allows the compiler to analyze coroutine functions and generate metadata that guides runtime allocation decisions.

This protocol achieves:
- **Static stack size analysis** with bounds checking
- **Optimal allocation strategy selection** based on compile-time analysis
- **Profile-guided optimization feedback** from runtime to compiler
- **Zero-overhead abstractions** through intelligent cooperation

## Compiler Analysis Phase

### Static Stack Analysis

The compiler performs comprehensive analysis of coroutine functions to determine memory requirements:

```cpp
// Example function for analysis
async fn web_request_handler(request: HttpRequest) -> HttpResponse {
    let headers = parse_headers(&request.headers);        // +128 bytes
    let auth_token = extract_auth(&headers);             // +64 bytes
    let user_data = co_await load_user_data(&auth_token); // +256 bytes
    let response = build_response(&user_data);           // +192 bytes
    response
}

// Compiler generates this analysis
struct StackAnalysis {
    function_id: FunctionId,
    local_variables_size: usize,        // 640 bytes total
    max_call_depth: u32,                // 4 levels deep
    recursive_calls: Vec<FunctionId>,   // None detected
    unbounded_loops: bool,              // false
    dynamic_allocations: Vec<AllocSite>, // Vec allocations detected
    suspension_points: Vec<SuspensionInfo>, // 1 co_await
}
```

### Call Graph Analysis

The compiler builds a complete call graph to understand stack growth patterns:

```cpp
// Compiler tracks call relationships
struct CallGraphNode {
    function_id: FunctionId,
    stack_frame_size: usize,
    callees: Vec<FunctionId>,
    max_depth: Option<u32>,  // None if unbounded recursion
    is_recursive: bool,
}

// Example analysis result
CallGraph {
    nodes: {
        web_request_handler: CallGraphNode {
            function_id: "web_request_handler",
            stack_frame_size: 640,
            callees: ["parse_headers", "extract_auth", "load_user_data", "build_response"],
            max_depth: Some(4),
            is_recursive: false,
        },
        parse_headers: CallGraphNode {
            function_id: "parse_headers", 
            stack_frame_size: 256,
            callees: ["header_parser_recursive"],
            max_depth: Some(10),  // Bounded recursion depth
            is_recursive: false,
        },
        header_parser_recursive: CallGraphNode {
            function_id: "header_parser_recursive",
            stack_frame_size: 128,
            callees: ["header_parser_recursive"],  // Self-recursive
            max_depth: Some(10),  // Compiler-enforced bound
            is_recursive: true,
        },
    }
}
```

### Size Classification Algorithm

```cpp
fn classify_coroutine_size(analysis: &StackAnalysis, call_graph: &CallGraph) -> CoroSizeTag {
    let total_stack_size = calculate_max_stack_usage(analysis, call_graph);
    let has_unbounded_recursion = detect_unbounded_recursion(call_graph);
    let has_dynamic_growth = analysis.unbounded_loops || !analysis.dynamic_allocations.is_empty();
    
    match (total_stack_size, has_unbounded_recursion, has_dynamic_growth) {
        (size, false, false) if size <= 256 => CoroSizeTag::Micro,
        (size, false, false) if size <= 2048 => CoroSizeTag::Small,
        (size, false, false) if size <= 16384 => CoroSizeTag::Medium,
        _ => CoroSizeTag::Large,
    }
}

fn calculate_max_stack_usage(analysis: &StackAnalysis, call_graph: &CallGraph) -> usize {
    let mut max_usage = 0;
    
    // Walk all possible execution paths
    for path in enumerate_execution_paths(call_graph, analysis.function_id) {
        let path_usage = path.iter()
            .map(|func_id| call_graph.nodes[func_id].stack_frame_size)
            .sum::<usize>();
        
        max_usage = max_usage.max(path_usage);
    }
    
    max_usage
}
```

## Metadata Generation

### Compile-Time Metadata Structure

```cpp
// Generated by compiler for each coroutine function
#[repr(C)]
struct CoroMetadata {
    // Basic size information
    function_id: u64,
    estimated_stack_size: usize,
    max_stack_size: Option<usize>,  // None if unbounded
    
    // Behavioral characteristics
    is_bounded: bool,
    is_recursive: bool,
    recursion_depth: Option<u32>,
    
    // Call graph information
    call_depth: u32,
    callee_count: u32,
    callee_ids: *const u64,
    
    // Allocation information
    dynamic_alloc_count: u32,
    suspension_point_count: u32,
    
    // Optimization hints
    size_class: CoroSizeTag,
    preferred_pool: PoolHint,
    migration_likelihood: f32,  // 0.0 to 1.0
}

// Automatically generated metadata table
#[link_section = ".coro_metadata"]
static COROUTINE_METADATA: &[CoroMetadata] = &[
    CoroMetadata {
        function_id: hash_of!("web_request_handler"),
        estimated_stack_size: 640,
        max_stack_size: Some(1024),
        is_bounded: true,
        is_recursive: false,
        recursion_depth: None,
        call_depth: 4,
        callee_count: 4,
        callee_ids: &[
            hash_of!("parse_headers"),
            hash_of!("extract_auth"), 
            hash_of!("load_user_data"),
            hash_of!("build_response")
        ] as *const u64,
        dynamic_alloc_count: 2,
        suspension_point_count: 1,
        size_class: CoroSizeTag::Small,
        preferred_pool: PoolHint::WebRequestPool,
        migration_likelihood: 0.1,
    },
    // ... more entries
];
```

### Metadata Access API

```cpp
// Library interface for accessing compiler-generated metadata
functional class MetadataRegistry {
    fn lookup_metadata(function_id: u64) -> Option<&'static CoroMetadata> {
        COROUTINE_METADATA.iter()
            .find(|meta| meta.function_id == function_id)
    }
    
    fn get_size_hint<F>() -> CoroSizeTag 
    where F: CoroFunction
    {
        let function_id = F::function_id();
        Self::lookup_metadata(function_id)
            .map(|meta| meta.size_class)
            .unwrap_or(CoroSizeTag::Medium)  // Conservative default
    }
    
    fn get_pool_hint<F>() -> PoolHint
    where F: CoroFunction  
    {
        let function_id = F::function_id();
        Self::lookup_metadata(function_id)
            .map(|meta| meta.preferred_pool)
            .unwrap_or(PoolHint::General)
    }
}
```

## Runtime Library Interface

### Allocation Strategy Selection

```cpp
// Library uses metadata to guide allocation decisions
functional class CoroAllocator {
    fn allocate_for_function<F>(args: F::Args) -> CoroHandle<F::Output>
    where F: CoroFunction
    {
        let metadata = MetadataRegistry::lookup_metadata(F::function_id())
            .expect("Missing metadata for coroutine function");
        
        let allocation_strategy = select_strategy(metadata);
        
        match allocation_strategy {
            AllocationStrategy::MicroPool(pool_id) => {
                Self::allocate_from_micro_pool(pool_id, metadata)
            },
            AllocationStrategy::SizedPool(size_class) => {
                Self::allocate_from_sized_pool(size_class, metadata)
            },
            AllocationStrategy::Individual => {
                Self::allocate_individual(metadata.estimated_stack_size * 2) // 2x safety margin
            },
            AllocationStrategy::Fallback => {
                Self::allocate_fallback(DEFAULT_CORO_SIZE)
            },
        }
    }
    
    fn select_strategy(metadata: &CoroMetadata) -> AllocationStrategy {
        // Use compiler analysis to choose optimal strategy
        match metadata.size_class {
            CoroSizeTag::Micro if metadata.migration_likelihood < 0.2 => {
                AllocationStrategy::MicroPool(select_micro_pool(metadata))
            },
            CoroSizeTag::Small | CoroSizeTag::Medium => {
                AllocationStrategy::SizedPool(metadata.size_class)
            },
            CoroSizeTag::Large | _ => {
                AllocationStrategy::Individual
            },
        }
    }
}

enum AllocationStrategy {
    MicroPool(PoolId),
    SizedPool(CoroSizeTag),
    Individual,
    Fallback,
}
```

### Pool Management with Metadata

```cpp
class CoroPoolManager {
    micro_pools: Vec<MicroPool>,
    sized_pools: HashMap<CoroSizeTag, SizedPool>,
    individual_allocations: HashMap<CoroId, IndividualAllocation>,
    metadata_stats: MetadataStats,
    
    constructed_by: PoolManagerOps,
}

functional class PoolManagerOps {
    fn construct_pools(config: &PoolConfig) -> CoroPoolManager {
        // Pre-allocate pools based on metadata analysis
        let metadata_analysis = analyze_all_metadata();
        
        let micro_pool_count = estimate_micro_usage(&metadata_analysis);
        let sized_pool_sizes = estimate_sized_usage(&metadata_analysis);
        
        CoroPoolManager {
            micro_pools: create_micro_pools(micro_pool_count),
            sized_pools: create_sized_pools(&sized_pool_sizes),
            individual_allocations: HashMap::new(),
            metadata_stats: MetadataStats::new(),
        }
    }
    
    fn allocate_with_metadata(
        manager: &mut CoroPoolManager,
        metadata: &CoroMetadata
    ) -> CoroAllocation {
        // Track metadata accuracy
        manager.metadata_stats.record_allocation_request(metadata);
        
        match metadata.size_class {
            CoroSizeTag::Micro => {
                if let Some(allocation) = try_allocate_micro(manager, metadata) {
                    allocation
                } else {
                    // Micro pool exhausted, fallback to small
                    manager.metadata_stats.record_fallback(metadata.function_id);
                    allocate_from_sized_pool(manager, CoroSizeTag::Small)
                }
            },
            size_class => allocate_from_sized_pool(manager, size_class),
        }
    }
}

// Metadata accuracy tracking
struct MetadataStats {
    allocation_counts: HashMap<u64, u32>,          // function_id -> count
    size_accuracy: HashMap<u64, SizeAccuracy>,    // function_id -> accuracy
    migration_events: HashMap<u64, u32>,          // function_id -> migrations
    fallback_events: HashMap<u64, u32>,           // function_id -> fallbacks
}

struct SizeAccuracy {
    predicted_size: usize,
    actual_sizes: Vec<usize>,
    average_actual: f64,
    accuracy_ratio: f64,  // actual / predicted
}
```

## Profile-Guided Optimization

### Runtime Feedback Collection

```cpp
// Runtime collects statistics for compiler feedback
class ProfileCollector {
    execution_profiles: HashMap<u64, ExecutionProfile>,
    size_measurements: HashMap<u64, SizeMeasurements>,
    migration_patterns: HashMap<u64, MigrationPattern>,
    
    constructed_by: ProfileOps,
}

struct ExecutionProfile {
    function_id: u64,
    total_executions: u64,
    average_runtime_us: f64,
    suspension_count_distribution: Histogram,
    stack_usage_distribution: Histogram,
}

struct SizeMeasurements {
    function_id: u64,
    predicted_size: usize,
    measured_sizes: Vec<usize>,
    max_observed_size: usize,
    migration_threshold_hit: bool,
}

functional class ProfileOps {
    fn record_execution(
        collector: &mut ProfileCollector,
        function_id: u64,
        runtime_us: f64,
        stack_usage: usize,
        suspension_count: u32
    ) {
        let profile = collector.execution_profiles
            .entry(function_id)
            .or_insert_with(|| ExecutionProfile::new(function_id));
        
        profile.total_executions += 1;
        profile.average_runtime_us = update_average(
            profile.average_runtime_us,
            runtime_us,
            profile.total_executions
        );
        profile.suspension_count_distribution.add(suspension_count);
        profile.stack_usage_distribution.add(stack_usage);
    }
    
    fn record_size_measurement(
        collector: &mut ProfileCollector,
        function_id: u64,
        predicted_size: usize,
        actual_size: usize
    ) {
        let measurements = collector.size_measurements
            .entry(function_id)
            .or_insert_with(|| SizeMeasurements::new(function_id, predicted_size));
        
        measurements.measured_sizes.push(actual_size);
        measurements.max_observed_size = measurements.max_observed_size.max(actual_size);
        
        if actual_size > predicted_size * 2 {
            measurements.migration_threshold_hit = true;
        }
    }
    
    fn generate_compiler_feedback(collector: &ProfileCollector) -> CompilerFeedback {
        let mut feedback = CompilerFeedback::new();
        
        for (function_id, measurements) in &collector.size_measurements {
            let average_actual = measurements.measured_sizes.iter().sum::<usize>() as f64
                / measurements.measured_sizes.len() as f64;
            
            let accuracy_ratio = average_actual / measurements.predicted_size as f64;
            
            if accuracy_ratio > 1.5 {
                // Consistently underestimating size
                feedback.underestimated_functions.push(UnderestimatedFunction {
                    function_id: *function_id,
                    predicted_size: measurements.predicted_size,
                    actual_average: average_actual as usize,
                    suggested_size: (average_actual * 1.2) as usize,  // 20% safety margin
                });
            } else if accuracy_ratio < 0.6 {
                // Consistently overestimating size  
                feedback.overestimated_functions.push(OverestimatedFunction {
                    function_id: *function_id,
                    predicted_size: measurements.predicted_size,
                    actual_average: average_actual as usize,
                    suggested_size: (average_actual * 1.1) as usize,  // 10% safety margin
                });
            }
        }
        
        feedback
    }
}
```

### Compiler Feedback Integration

```cpp
// Feedback structure sent back to compiler
struct CompilerFeedback {
    underestimated_functions: Vec<UnderestimatedFunction>,
    overestimated_functions: Vec<OverestimatedFunction>,
    migration_hotspots: Vec<MigrationHotspot>,
    new_size_class_suggestions: Vec<SizeClassSuggestion>,
}

struct UnderestimatedFunction {
    function_id: u64,
    predicted_size: usize,
    actual_average: usize,
    suggested_size: usize,
    confidence: f64,  // 0.0 to 1.0
}

struct MigrationHotspot {
    function_id: u64,
    migration_frequency: f64,  // migrations per execution
    suggested_initial_class: CoroSizeTag,
    reason: MigrationReason,
}

enum MigrationReason {
    UnboundedRecursionDetected,
    DynamicAllocationGrowth,
    CallDepthUnderestimated,
    StackFrameSizeIncreased,
}

// Compiler processes feedback for next compilation
fn process_runtime_feedback(feedback: &CompilerFeedback) -> UpdatedAnalysis {
    let mut updated_metadata = HashMap::new();
    
    for underestimated in &feedback.underestimated_functions {
        if underestimated.confidence > 0.8 {
            // High confidence feedback - update size estimate
            updated_metadata.insert(underestimated.function_id, MetadataUpdate {
                new_estimated_size: underestimated.suggested_size,
                new_size_class: classify_size(underestimated.suggested_size),
                analysis_adjustment: AnalysisAdjustment::IncreaseStackEstimate,
            });
        }
    }
    
    for hotspot in &feedback.migration_hotspots {
        // Functions that frequently migrate should start in larger class
        updated_metadata.insert(hotspot.function_id, MetadataUpdate {
            new_size_class: hotspot.suggested_initial_class,
            analysis_adjustment: match hotspot.reason {
                MigrationReason::UnboundedRecursionDetected => {
                    AnalysisAdjustment::MarkAsUnbounded
                },
                MigrationReason::DynamicAllocationGrowth => {
                    AnalysisAdjustment::AccountForDynamicGrowth
                },
                _ => AnalysisAdjustment::IncreaseConservatism,
            },
            ..Default::default()
        });
    }
    
    UpdatedAnalysis { updated_metadata }
}
```

## Advanced Optimization Techniques

### Cross-Module Analysis

```cpp
// Compiler can analyze coroutine usage across module boundaries
struct CrossModuleAnalysis {
    call_graph: GlobalCallGraph,
    size_propagation: SizePropagationAnalysis,
    usage_patterns: UsagePatternAnalysis,
}

// Example: Library function called from multiple modules
mod http_library {
    // This function is called from many different modules
    pub async fn parse_http_request(raw_request: &[u8]) -> HttpRequest {
        // Compiler sees all call sites across the entire program
        // and can optimize for the most common usage patterns
        let headers = parse_headers(raw_request)?;
        let body = parse_body(raw_request, &headers)?;
        HttpRequest { headers, body }
    }
}

// Compiler analysis across all call sites
CrossModuleUsageAnalysis {
    function_id: hash_of!("parse_http_request"),
    call_sites: vec![
        CallSite { module: "web_server", frequency: 10000, avg_input_size: 2048 },
        CallSite { module: "api_gateway", frequency: 50000, avg_input_size: 1024 },
        CallSite { module: "proxy", frequency: 100000, avg_input_size: 512 },
    ],
    // Optimize for the most common case (proxy with 512B inputs)
    recommended_size_class: CoroSizeTag::Small,
}
```

### Adaptive Pool Sizing

```cpp
// Pools that adapt based on runtime measurements
class AdaptivePoolManager {
    base_pools: HashMap<CoroSizeTag, BasePool>,
    adaptive_pools: HashMap<u64, AdaptivePool>,  // function_id -> specialized pool
    allocation_history: CircularBuffer<AllocationEvent>,
    
    constructed_by: AdaptivePoolOps,
}

struct AdaptivePool {
    function_id: u64,
    current_size: usize,
    allocation_count: u64,
    hit_rate: f64,
    resize_trigger: ResizeTrigger,
}

functional class AdaptivePoolOps {
    fn allocate_adaptive(
        manager: &mut AdaptivePoolManager,
        function_id: u64,
        metadata: &CoroMetadata
    ) -> CoroAllocation {
        // Check if this function has a specialized adaptive pool
        if let Some(adaptive_pool) = manager.adaptive_pools.get_mut(&function_id) {
            if adaptive_pool.should_resize() {
                Self::resize_adaptive_pool(adaptive_pool, &manager.allocation_history);
            }
            
            Self::allocate_from_adaptive_pool(adaptive_pool)
        } else {
            // Check if function should get its own adaptive pool
            let usage_frequency = calculate_usage_frequency(function_id, &manager.allocation_history);
            
            if usage_frequency > ADAPTIVE_POOL_THRESHOLD {
                let new_pool = Self::create_adaptive_pool(function_id, metadata);
                manager.adaptive_pools.insert(function_id, new_pool);
                Self::allocate_from_adaptive_pool(&manager.adaptive_pools[&function_id])
            } else {
                // Use general pool
                Self::allocate_from_base_pool(manager, metadata.size_class)
            }
        }
    }
    
    fn resize_adaptive_pool(pool: &mut AdaptivePool, history: &CircularBuffer<AllocationEvent>) {
        let recent_allocations = history.iter()
            .filter(|event| event.function_id == pool.function_id)
            .take(1000)  // Last 1000 allocations
            .collect::<Vec<_>>();
        
        let average_actual_size = recent_allocations.iter()
            .map(|event| event.actual_stack_usage)
            .sum::<usize>() / recent_allocations.len();
        
        let new_size = (average_actual_size as f64 * 1.2) as usize;  // 20% safety margin
        
        if new_size > pool.current_size * 1.5 || new_size < pool.current_size * 0.7 {
            // Significant change - resize the pool
            pool.current_size = new_size;
            pool.resize_trigger = ResizeTrigger::StatisticalAnalysis;
        }
    }
}
```

### Just-In-Time Pool Creation

```cpp
// Create specialized pools for hot functions at runtime
functional class JITPoolManager {
    fn monitor_allocation_patterns(
        base_manager: &mut CoroPoolManager,
        allocation_stream: &mut AllocationStream
    ) {
        let mut hot_functions = HashMap::new();
        
        for allocation_event in allocation_stream {
            let counter = hot_functions.entry(allocation_event.function_id).or_insert(0u64);
            *counter += 1;
            
            // If function becomes hot, create specialized pool
            if *counter == JIT_POOL_CREATION_THRESHOLD {
                let specialized_pool = Self::create_jit_pool(
                    allocation_event.function_id,
                    &allocation_event.metadata
                );
                
                base_manager.add_specialized_pool(
                    allocation_event.function_id,
                    specialized_pool
                );
                
                log::info!(
                    "Created JIT pool for function {} after {} allocations",
                    allocation_event.function_id,
                    *counter
                );
            }
        }
    }
    
    fn create_jit_pool(function_id: u64, metadata: &CoroMetadata) -> SpecializedPool {
        // Analyze recent allocations for this function
        let recent_usage = analyze_recent_usage(function_id);
        
        let pool_size = calculate_optimal_pool_size(&recent_usage);
        let element_size = calculate_optimal_element_size(&recent_usage, metadata);
        
        SpecializedPool::new(
            function_id,
            pool_size,
            element_size,
            PoolOptimization::JITCreated
        )
    }
}
```

## Debugging and Profiling Integration

### Metadata-Aware Debugging

```cpp
// Debug information enhanced with compiler metadata
struct DebugCoroInfo {
    coroutine_id: CoroId,
    function_name: &'static str,
    function_id: u64,
    
    // From compiler metadata
    predicted_stack_size: usize,
    predicted_size_class: CoroSizeTag,
    
    // From runtime
    actual_stack_size: usize,
    actual_allocation_source: AllocationSource,
    migration_history: Vec<MigrationEvent>,
    
    // Performance data
    allocation_time_ns: u64,
    context_switches: u64,
    total_runtime_us: u64,
}

functional class CoroDebugger {
    fn print_coroutine_info(coro_id: CoroId) {
        let info = get_coro_debug_info(coro_id);
        
        println!("Coroutine Debug Info:");
        println!("  Function: {} (ID: {})", info.function_name, info.function_id);
        println!("  Predicted size: {} bytes ({})", 
                info.predicted_stack_size, 
                info.predicted_size_class);
        println!("  Actual size: {} bytes", info.actual_stack_size);
        println!("  Allocation source: {:?}", info.actual_allocation_source);
        
        if info.actual_stack_size as f64 / info.predicted_stack_size as f64 > 1.5 {
            println!("  ⚠️  WARNING: Actual size significantly exceeds prediction");
            println!("     Consider updating compiler analysis or using larger size class");
        }
        
        if !info.migration_history.is_empty() {
            println!("  Migration history:");
            for migration in &info.migration_history {
                println!("    {} -> {} (reason: {:?})",
                        migration.from_class,
                        migration.to_class, 
                        migration.reason);
            }
        }
    }
    
    fn validate_metadata_accuracy() -> MetadataValidationReport {
        let mut report = MetadataValidationReport::new();
        
        for (function_id, stats) in get_all_function_stats() {
            let metadata = MetadataRegistry::lookup_metadata(function_id)
                .expect("Missing metadata");
            
            let accuracy = calculate_size_accuracy(&stats, metadata);
            
            if accuracy < ACCURACY_THRESHOLD {
                report.inaccurate_predictions.push(InaccuratePrediction {
                    function_id,
                    predicted_size: metadata.estimated_stack_size,
                    actual_average: stats.average_stack_usage,
                    sample_count: stats.execution_count,
                    accuracy_ratio: accuracy,
                });
            }
        }
        
        report
    }
}
```

### Performance Profiling Integration

```cpp
// Integration with standard profiling tools
class CoroProfilerIntegration {
    vtune_integration: Option<VTuneIntegration>,
    perf_integration: Option<PerfIntegration>, 
    custom_profiler: Option<CustomProfiler>,
    
    constructed_by: ProfilerIntegrationOps,
}

functional class ProfilerIntegrationOps {
    fn start_profiling_session(integration: &mut CoroProfilerIntegration) {
        // Notify profiler about coroutine metadata
        if let Some(ref mut vtune) = integration.vtune_integration {
            for metadata in COROUTINE_METADATA {
                vtune.register_coroutine_function(
                    metadata.function_id,
                    metadata.estimated_stack_size,
                    metadata.size_class
                );
            }
        }
        
        // Set up custom markers for different allocation paths
        Self::setup_allocation_markers(integration);
    }
    
    fn record_allocation_event(
        integration: &CoroProfilerIntegration,
        function_id: u64,
        allocation_source: AllocationSource,
        allocation_time_ns: u64
    ) {
        // Record in all active profilers
        if let Some(ref vtune) = integration.vtune_integration {
            vtune.record_custom_event("coro_allocation", function_id, allocation_time_ns);
        }
        
        if let Some(ref perf) = integration.perf_integration {
            perf.record_allocation_marker(function_id, allocation_source);
        }
    }
}
```

## Best Practices for Protocol Usage

### Compiler Implementation Guidelines

```cpp
// Recommended compiler analysis depth
struct AnalysisConfiguration {
    max_call_depth: u32,           // 50 levels recommended
    max_recursion_analysis: u32,   // 100 iterations recommended
    enable_cross_module: bool,     // true for whole-program optimization
    conservative_fallback: bool,   // true for production, false for profiling
    
    // Size estimation safety margins
    stack_size_safety_factor: f64, // 1.2 recommended (20% safety margin)
    recursion_safety_factor: f64,  // 2.0 recommended (100% safety margin)
}

// Example of conservative vs aggressive analysis
fn analyze_coroutine_function(func: &Function, config: &AnalysisConfiguration) -> CoroMetadata {
    let base_analysis = perform_static_analysis(func);
    
    let estimated_size = if config.conservative_fallback {
        // Conservative: assume worst case for ambiguous situations
        base_analysis.max_case_size * config.stack_size_safety_factor
    } else {
        // Aggressive: use average case, rely on runtime feedback
        base_analysis.average_case_size * config.stack_size_safety_factor
    };
    
    let size_class = if estimated_size <= 256 && base_analysis.is_definitely_bounded {
        CoroSizeTag::Micro
    } else if estimated_size <= 2048 && base_analysis.bounded_with_high_confidence {
        CoroSizeTag::Small
    } else if estimated_size <= 16384 && base_analysis.probably_bounded {
        CoroSizeTag::Medium
    } else {
        CoroSizeTag::Large
    };
    
    CoroMetadata {
        function_id: func.id(),
        estimated_stack_size: estimated_size as usize,
        size_class,
        is_bounded: base_analysis.is_definitely_bounded,
        migration_likelihood: calculate_migration_likelihood(&base_analysis),
        ..Default::default()
    }
}
```

### Library Implementation Guidelines

```cpp
// Recommended pool configuration
struct RecommendedPoolConfig {
    // Conservative defaults for general-purpose applications
    micro_pool_count: usize,      // 4 pools of 1000 slots each
    micro_pool_size: usize,       // 256KB per pool (1000 * 256B)
    
    small_pool_count: usize,      // 2 pools
    small_pool_capacity: usize,   // 500 slots per pool (1MB per pool)
    
    medium_pool_count: usize,     // 1 pool
    medium_pool_capacity: usize,  // 100 slots (1.6MB pool)
    
    // Individual allocation for Large class
    large_allocation_tracking: bool, // true - track for optimization
}

fn create_production_pool_config() -> RecommendedPoolConfig {
    RecommendedPoolConfig {
        micro_pool_count: determine_micro_pool_count(),
        micro_pool_size: 256 * 1000,  // 1000 micro coroutines per pool
        
        small_pool_count: 2,
        small_pool_capacity: 500,
        
        medium_pool_count: 1,
        medium_pool_capacity: 100,
        
        large_allocation_tracking: true,
    }
}

fn determine_micro_pool_count() -> usize {
    // Base on available memory and expected workload
    let available_memory = get_available_memory();
    let expected_micro_coros = estimate_concurrent_micro_coroutines();
    
    let pools_needed = (expected_micro_coros + 999) / 1000;  // Round up
    let max_pools_by_memory = available_memory / (256 * 1024);  // 256KB per pool
    
    pools_needed.min(max_pools_by_memory).max(1)  // At least 1 pool
}
```

The CPrime compiler-library protocol represents a significant innovation in cooperative multitasking systems, enabling unprecedented cooperation between static analysis and runtime optimization to achieve both safety and performance.