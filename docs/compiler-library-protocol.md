# CPrime Compiler-Library Protocol

## Overview

The CPrime coroutine system introduces a revolutionary **compiler-library cooperation protocol** that bridges static analysis with runtime optimization. Unlike traditional approaches where the compiler or runtime operates independently, CPrime's design allows the compiler to analyze coroutine functions and generate metadata that guides runtime allocation decisions.

This protocol achieves:
- **Static stack size analysis** with bounds checking
- **Optimal allocation strategy selection** based on compile-time analysis
- **Profile-guided optimization feedback** from runtime to compiler
- **Zero-overhead abstractions** through intelligent cooperation

## Compiler Analysis Phase

### Static Stack Analysis

The compiler performs comprehensive analysis of coroutine functions to determine memory requirements:

```cpp
// Example function for analysis
async fn web_request_handler(request: HttpRequest) -> HttpResponse {
    let headers = parse_headers(&request.headers);        // +128 bytes
    let auth_token = extract_auth(&headers);             // +64 bytes
    let user_data = co_await load_user_data(&auth_token); // +256 bytes
    let response = build_response(&user_data);           // +192 bytes
    response
}

// Compiler generates this analysis
struct StackAnalysis {
    function_id: FunctionId,
    local_variables_size: usize,        // 640 bytes total
    max_call_depth: u32,                // 4 levels deep
    recursive_calls: Vec<FunctionId>,   // None detected
    unbounded_loops: bool,              // false
    dynamic_allocations: Vec<AllocSite>, // Vec allocations detected
    suspension_points: Vec<SuspensionInfo>, // 1 co_await
}
```

### Call Graph Analysis

The compiler builds a complete call graph to understand stack growth patterns:

```cpp
// Compiler tracks call relationships
struct CallGraphNode {
    function_id: FunctionId,
    stack_frame_size: usize,
    callees: Vec<FunctionId>,
    max_depth: Option<u32>,  // None if unbounded recursion
    is_recursive: bool,
}

// Example analysis result
CallGraph {
    nodes: {
        web_request_handler: CallGraphNode {
            function_id: "web_request_handler",
            stack_frame_size: 640,
            callees: ["parse_headers", "extract_auth", "load_user_data", "build_response"],
            max_depth: Some(4),
            is_recursive: false,
        },
        parse_headers: CallGraphNode {
            function_id: "parse_headers", 
            stack_frame_size: 256,
            callees: ["header_parser_recursive"],
            max_depth: Some(10),  // Bounded recursion depth
            is_recursive: false,
        },
        header_parser_recursive: CallGraphNode {
            function_id: "header_parser_recursive",
            stack_frame_size: 128,
            callees: ["header_parser_recursive"],  // Self-recursive
            max_depth: Some(10),  // Compiler-enforced bound
            is_recursive: true,
        },
    }
}
```

### Size Classification Algorithm

```cpp
fn classify_coroutine_size(analysis: &StackAnalysis, call_graph: &CallGraph) -> CoroSizeTag {
    let total_stack_size = calculate_max_stack_usage(analysis, call_graph);
    let has_unbounded_recursion = detect_unbounded_recursion(call_graph);
    let has_dynamic_growth = analysis.unbounded_loops || !analysis.dynamic_allocations.is_empty();
    
    match (total_stack_size, has_unbounded_recursion, has_dynamic_growth) {
        (size, false, false) if size <= 256 => CoroSizeTag::Micro,
        (size, false, false) if size <= 2048 => CoroSizeTag::Small,
        (size, false, false) if size <= 16384 => CoroSizeTag::Medium,
        _ => CoroSizeTag::Large,
    }
}

fn calculate_max_stack_usage(analysis: &StackAnalysis, call_graph: &CallGraph) -> usize {
    let mut max_usage = 0;
    
    // Walk all possible execution paths
    for path in enumerate_execution_paths(call_graph, analysis.function_id) {
        let path_usage = path.iter()
            .map(|func_id| call_graph.nodes[func_id].stack_frame_size)
            .sum::<usize>();
        
        max_usage = max_usage.max(path_usage);
    }
    
    max_usage
}
```

## Metadata Generation

### Compile-Time Metadata Structure

```cpp
// Generated by compiler for each coroutine function
#[repr(C)]
struct CoroMetadata {
    // Basic size information
    function_id: u64,
    estimated_stack_size: usize,
    max_stack_size: Option<usize>,  // None if unbounded
    
    // Behavioral characteristics
    is_bounded: bool,
    is_recursive: bool,
    recursion_depth: Option<u32>,
    
    // Call graph information
    call_depth: u32,
    callee_count: u32,
    callee_ids: *const u64,
    
    // Allocation information
    dynamic_alloc_count: u32,
    suspension_point_count: u32,
    
    // Optimization hints
    size_class: CoroSizeTag,
    preferred_pool: PoolHint,
    migration_likelihood: f32,  // 0.0 to 1.0
}

// Automatically generated metadata table
#[link_section = ".coro_metadata"]
static COROUTINE_METADATA: &[CoroMetadata] = &[
    CoroMetadata {
        function_id: hash_of!("web_request_handler"),
        estimated_stack_size: 640,
        max_stack_size: Some(1024),
        is_bounded: true,
        is_recursive: false,
        recursion_depth: None,
        call_depth: 4,
        callee_count: 4,
        callee_ids: &[
            hash_of!("parse_headers"),
            hash_of!("extract_auth"), 
            hash_of!("load_user_data"),
            hash_of!("build_response")
        ] as *const u64,
        dynamic_alloc_count: 2,
        suspension_point_count: 1,
        size_class: CoroSizeTag::Small,
        preferred_pool: PoolHint::WebRequestPool,
        migration_likelihood: 0.1,
    },
    // ... more entries
];
```

### Metadata Access API

```cpp
// Library interface for accessing compiler-generated metadata
functional class MetadataRegistry {
    fn lookup_metadata(function_id: u64) -> Option<&'static CoroMetadata> {
        COROUTINE_METADATA.iter()
            .find(|meta| meta.function_id == function_id)
    }
    
    fn get_size_hint<F>() -> CoroSizeTag 
    where F: CoroFunction
    {
        let function_id = F::function_id();
        Self::lookup_metadata(function_id)
            .map(|meta| meta.size_class)
            .unwrap_or(CoroSizeTag::Medium)  // Conservative default
    }
    
    fn get_pool_hint<F>() -> PoolHint
    where F: CoroFunction  
    {
        let function_id = F::function_id();
        Self::lookup_metadata(function_id)
            .map(|meta| meta.preferred_pool)
            .unwrap_or(PoolHint::General)
    }
}
```

## Runtime Library Interface

### Allocation Strategy Selection

```cpp
// Library uses metadata to guide allocation decisions
functional class CoroAllocator {
    fn allocate_for_function<F>(args: F::Args) -> CoroHandle<F::Output>
    where F: CoroFunction
    {
        let metadata = MetadataRegistry::lookup_metadata(F::function_id())
            .expect("Missing metadata for coroutine function");
        
        let allocation_strategy = select_strategy(metadata);
        
        match allocation_strategy {
            AllocationStrategy::MicroPool(pool_id) => {
                Self::allocate_from_micro_pool(pool_id, metadata)
            },
            AllocationStrategy::SizedPool(size_class) => {
                Self::allocate_from_sized_pool(size_class, metadata)
            },
            AllocationStrategy::Individual => {
                Self::allocate_individual(metadata.estimated_stack_size * 2) // 2x safety margin
            },
            AllocationStrategy::Fallback => {
                Self::allocate_fallback(DEFAULT_CORO_SIZE)
            },
        }
    }
    
    fn select_strategy(metadata: &CoroMetadata) -> AllocationStrategy {
        // Use compiler analysis to choose optimal strategy
        match metadata.size_class {
            CoroSizeTag::Micro if metadata.migration_likelihood < 0.2 => {
                AllocationStrategy::MicroPool(select_micro_pool(metadata))
            },
            CoroSizeTag::Small | CoroSizeTag::Medium => {
                AllocationStrategy::SizedPool(metadata.size_class)
            },
            CoroSizeTag::Large | _ => {
                AllocationStrategy::Individual
            },
        }
    }
}

enum AllocationStrategy {
    MicroPool(PoolId),
    SizedPool(CoroSizeTag),
    Individual,
    Fallback,
}
```

### Pool Management with Metadata

```cpp
class CoroPoolManager {
    micro_pools: Vec<MicroPool>,
    sized_pools: HashMap<CoroSizeTag, SizedPool>,
    individual_allocations: HashMap<CoroId, IndividualAllocation>,
    metadata_stats: MetadataStats,
    
    constructed_by: PoolManagerOps,
}

functional class PoolManagerOps {
    fn construct_pools(config: &PoolConfig) -> CoroPoolManager {
        // Pre-allocate pools based on metadata analysis
        let metadata_analysis = analyze_all_metadata();
        
        let micro_pool_count = estimate_micro_usage(&metadata_analysis);
        let sized_pool_sizes = estimate_sized_usage(&metadata_analysis);
        
        CoroPoolManager {
            micro_pools: create_micro_pools(micro_pool_count),
            sized_pools: create_sized_pools(&sized_pool_sizes),
            individual_allocations: HashMap::new(),
            metadata_stats: MetadataStats::new(),
        }
    }
    
    fn allocate_with_metadata(
        manager: &mut CoroPoolManager,
        metadata: &CoroMetadata
    ) -> CoroAllocation {
        // Track metadata accuracy
        manager.metadata_stats.record_allocation_request(metadata);
        
        match metadata.size_class {
            CoroSizeTag::Micro => {
                if let Some(allocation) = try_allocate_micro(manager, metadata) {
                    allocation
                } else {
                    // Micro pool exhausted, fallback to small
                    manager.metadata_stats.record_fallback(metadata.function_id);
                    allocate_from_sized_pool(manager, CoroSizeTag::Small)
                }
            },
            size_class => allocate_from_sized_pool(manager, size_class),
        }
    }
}

// Metadata accuracy tracking
struct MetadataStats {
    allocation_counts: HashMap<u64, u32>,          // function_id -> count
    size_accuracy: HashMap<u64, SizeAccuracy>,    // function_id -> accuracy
    migration_events: HashMap<u64, u32>,          // function_id -> migrations
    fallback_events: HashMap<u64, u32>,           // function_id -> fallbacks
}

struct SizeAccuracy {
    predicted_size: usize,
    actual_sizes: Vec<usize>,
    average_actual: f64,
    accuracy_ratio: f64,  // actual / predicted
}
```

## Profile-Guided Optimization

### Runtime Feedback Collection

```cpp
// Runtime collects statistics for compiler feedback
class ProfileCollector {
    execution_profiles: HashMap<u64, ExecutionProfile>,
    size_measurements: HashMap<u64, SizeMeasurements>,
    migration_patterns: HashMap<u64, MigrationPattern>,
    
    constructed_by: ProfileOps,
}

struct ExecutionProfile {
    function_id: u64,
    total_executions: u64,
    average_runtime_us: f64,
    suspension_count_distribution: Histogram,
    stack_usage_distribution: Histogram,
}

struct SizeMeasurements {
    function_id: u64,
    predicted_size: usize,
    measured_sizes: Vec<usize>,
    max_observed_size: usize,
    migration_threshold_hit: bool,
}

functional class ProfileOps {
    fn record_execution(
        collector: &mut ProfileCollector,
        function_id: u64,
        runtime_us: f64,
        stack_usage: usize,
        suspension_count: u32
    ) {
        let profile = collector.execution_profiles
            .entry(function_id)
            .or_insert_with(|| ExecutionProfile::new(function_id));
        
        profile.total_executions += 1;
        profile.average_runtime_us = update_average(
            profile.average_runtime_us,
            runtime_us,
            profile.total_executions
        );
        profile.suspension_count_distribution.add(suspension_count);
        profile.stack_usage_distribution.add(stack_usage);
    }
    
    fn record_size_measurement(
        collector: &mut ProfileCollector,
        function_id: u64,
        predicted_size: usize,
        actual_size: usize
    ) {
        let measurements = collector.size_measurements
            .entry(function_id)
            .or_insert_with(|| SizeMeasurements::new(function_id, predicted_size));
        
        measurements.measured_sizes.push(actual_size);
        measurements.max_observed_size = measurements.max_observed_size.max(actual_size);
        
        if actual_size > predicted_size * 2 {
            measurements.migration_threshold_hit = true;
        }
    }
    
    fn generate_compiler_feedback(collector: &ProfileCollector) -> CompilerFeedback {
        let mut feedback = CompilerFeedback::new();
        
        for (function_id, measurements) in &collector.size_measurements {
            let average_actual = measurements.measured_sizes.iter().sum::<usize>() as f64
                / measurements.measured_sizes.len() as f64;
            
            let accuracy_ratio = average_actual / measurements.predicted_size as f64;
            
            if accuracy_ratio > 1.5 {
                // Consistently underestimating size
                feedback.underestimated_functions.push(UnderestimatedFunction {
                    function_id: *function_id,
                    predicted_size: measurements.predicted_size,
                    actual_average: average_actual as usize,
                    suggested_size: (average_actual * 1.2) as usize,  // 20% safety margin
                });
            } else if accuracy_ratio < 0.6 {
                // Consistently overestimating size  
                feedback.overestimated_functions.push(OverestimatedFunction {
                    function_id: *function_id,
                    predicted_size: measurements.predicted_size,
                    actual_average: average_actual as usize,
                    suggested_size: (average_actual * 1.1) as usize,  // 10% safety margin
                });
            }
        }
        
        feedback
    }
}
```

### Compiler Feedback Integration

```cpp
// Feedback structure sent back to compiler
struct CompilerFeedback {
    underestimated_functions: Vec<UnderestimatedFunction>,
    overestimated_functions: Vec<OverestimatedFunction>,
    migration_hotspots: Vec<MigrationHotspot>,
    new_size_class_suggestions: Vec<SizeClassSuggestion>,
}

struct UnderestimatedFunction {
    function_id: u64,
    predicted_size: usize,
    actual_average: usize,
    suggested_size: usize,
    confidence: f64,  // 0.0 to 1.0
}

struct MigrationHotspot {
    function_id: u64,
    migration_frequency: f64,  // migrations per execution
    suggested_initial_class: CoroSizeTag,
    reason: MigrationReason,
}

enum MigrationReason {
    UnboundedRecursionDetected,
    DynamicAllocationGrowth,
    CallDepthUnderestimated,
    StackFrameSizeIncreased,
}

// Compiler processes feedback for next compilation
fn process_runtime_feedback(feedback: &CompilerFeedback) -> UpdatedAnalysis {
    let mut updated_metadata = HashMap::new();
    
    for underestimated in &feedback.underestimated_functions {
        if underestimated.confidence > 0.8 {
            // High confidence feedback - update size estimate
            updated_metadata.insert(underestimated.function_id, MetadataUpdate {
                new_estimated_size: underestimated.suggested_size,
                new_size_class: classify_size(underestimated.suggested_size),
                analysis_adjustment: AnalysisAdjustment::IncreaseStackEstimate,
            });
        }
    }
    
    for hotspot in &feedback.migration_hotspots {
        // Functions that frequently migrate should start in larger class
        updated_metadata.insert(hotspot.function_id, MetadataUpdate {
            new_size_class: hotspot.suggested_initial_class,
            analysis_adjustment: match hotspot.reason {
                MigrationReason::UnboundedRecursionDetected => {
                    AnalysisAdjustment::MarkAsUnbounded
                },
                MigrationReason::DynamicAllocationGrowth => {
                    AnalysisAdjustment::AccountForDynamicGrowth
                },
                _ => AnalysisAdjustment::IncreaseConservatism,
            },
            ..Default::default()
        });
    }
    
    UpdatedAnalysis { updated_metadata }
}
```

## Advanced Optimization Techniques

### Cross-Module Analysis

```cpp
// Compiler can analyze coroutine usage across module boundaries
struct CrossModuleAnalysis {
    call_graph: GlobalCallGraph,
    size_propagation: SizePropagationAnalysis,
    usage_patterns: UsagePatternAnalysis,
}

// Example: Library function called from multiple modules
mod http_library {
    // This function is called from many different modules
    pub async fn parse_http_request(raw_request: &[u8]) -> HttpRequest {
        // Compiler sees all call sites across the entire program
        // and can optimize for the most common usage patterns
        let headers = parse_headers(raw_request)?;
        let body = parse_body(raw_request, &headers)?;
        HttpRequest { headers, body }
    }
}

// Compiler analysis across all call sites
CrossModuleUsageAnalysis {
    function_id: hash_of!("parse_http_request"),
    call_sites: vec![
        CallSite { module: "web_server", frequency: 10000, avg_input_size: 2048 },
        CallSite { module: "api_gateway", frequency: 50000, avg_input_size: 1024 },
        CallSite { module: "proxy", frequency: 100000, avg_input_size: 512 },
    ],
    // Optimize for the most common case (proxy with 512B inputs)
    recommended_size_class: CoroSizeTag::Small,
}
```

### Adaptive Pool Sizing

```cpp
// Pools that adapt based on runtime measurements
class AdaptivePoolManager {
    base_pools: HashMap<CoroSizeTag, BasePool>,
    adaptive_pools: HashMap<u64, AdaptivePool>,  // function_id -> specialized pool
    allocation_history: CircularBuffer<AllocationEvent>,
    
    constructed_by: AdaptivePoolOps,
}

struct AdaptivePool {
    function_id: u64,
    current_size: usize,
    allocation_count: u64,
    hit_rate: f64,
    resize_trigger: ResizeTrigger,
}

functional class AdaptivePoolOps {
    fn allocate_adaptive(
        manager: &mut AdaptivePoolManager,
        function_id: u64,
        metadata: &CoroMetadata
    ) -> CoroAllocation {
        // Check if this function has a specialized adaptive pool
        if let Some(adaptive_pool) = manager.adaptive_pools.get_mut(&function_id) {
            if adaptive_pool.should_resize() {
                Self::resize_adaptive_pool(adaptive_pool, &manager.allocation_history);
            }
            
            Self::allocate_from_adaptive_pool(adaptive_pool)
        } else {
            // Check if function should get its own adaptive pool
            let usage_frequency = calculate_usage_frequency(function_id, &manager.allocation_history);
            
            if usage_frequency > ADAPTIVE_POOL_THRESHOLD {
                let new_pool = Self::create_adaptive_pool(function_id, metadata);
                manager.adaptive_pools.insert(function_id, new_pool);
                Self::allocate_from_adaptive_pool(&manager.adaptive_pools[&function_id])
            } else {
                // Use general pool
                Self::allocate_from_base_pool(manager, metadata.size_class)
            }
        }
    }
    
    fn resize_adaptive_pool(pool: &mut AdaptivePool, history: &CircularBuffer<AllocationEvent>) {
        let recent_allocations = history.iter()
            .filter(|event| event.function_id == pool.function_id)
            .take(1000)  // Last 1000 allocations
            .collect::<Vec<_>>();
        
        let average_actual_size = recent_allocations.iter()
            .map(|event| event.actual_stack_usage)
            .sum::<usize>() / recent_allocations.len();
        
        let new_size = (average_actual_size as f64 * 1.2) as usize;  // 20% safety margin
        
        if new_size > pool.current_size * 1.5 || new_size < pool.current_size * 0.7 {
            // Significant change - resize the pool
            pool.current_size = new_size;
            pool.resize_trigger = ResizeTrigger::StatisticalAnalysis;
        }
    }
}
```

### Just-In-Time Pool Creation

```cpp
// Create specialized pools for hot functions at runtime
functional class JITPoolManager {
    fn monitor_allocation_patterns(
        base_manager: &mut CoroPoolManager,
        allocation_stream: &mut AllocationStream
    ) {
        let mut hot_functions = HashMap::new();
        
        for allocation_event in allocation_stream {
            let counter = hot_functions.entry(allocation_event.function_id).or_insert(0u64);
            *counter += 1;
            
            // If function becomes hot, create specialized pool
            if *counter == JIT_POOL_CREATION_THRESHOLD {
                let specialized_pool = Self::create_jit_pool(
                    allocation_event.function_id,
                    &allocation_event.metadata
                );
                
                base_manager.add_specialized_pool(
                    allocation_event.function_id,
                    specialized_pool
                );
                
                log::info!(
                    "Created JIT pool for function {} after {} allocations",
                    allocation_event.function_id,
                    *counter
                );
            }
        }
    }
    
    fn create_jit_pool(function_id: u64, metadata: &CoroMetadata) -> SpecializedPool {
        // Analyze recent allocations for this function
        let recent_usage = analyze_recent_usage(function_id);
        
        let pool_size = calculate_optimal_pool_size(&recent_usage);
        let element_size = calculate_optimal_element_size(&recent_usage, metadata);
        
        SpecializedPool::new(
            function_id,
            pool_size,
            element_size,
            PoolOptimization::JITCreated
        )
    }
}
```

## Debugging and Profiling Integration

### Metadata-Aware Debugging

```cpp
// Debug information enhanced with compiler metadata
struct DebugCoroInfo {
    coroutine_id: CoroId,
    function_name: &'static str,
    function_id: u64,
    
    // From compiler metadata
    predicted_stack_size: usize,
    predicted_size_class: CoroSizeTag,
    
    // From runtime
    actual_stack_size: usize,
    actual_allocation_source: AllocationSource,
    migration_history: Vec<MigrationEvent>,
    
    // Performance data
    allocation_time_ns: u64,
    context_switches: u64,
    total_runtime_us: u64,
}

functional class CoroDebugger {
    fn print_coroutine_info(coro_id: CoroId) {
        let info = get_coro_debug_info(coro_id);
        
        println!("Coroutine Debug Info:");
        println!("  Function: {} (ID: {})", info.function_name, info.function_id);
        println!("  Predicted size: {} bytes ({})", 
                info.predicted_stack_size, 
                info.predicted_size_class);
        println!("  Actual size: {} bytes", info.actual_stack_size);
        println!("  Allocation source: {:?}", info.actual_allocation_source);
        
        if info.actual_stack_size as f64 / info.predicted_stack_size as f64 > 1.5 {
            println!("  ⚠️  WARNING: Actual size significantly exceeds prediction");
            println!("     Consider updating compiler analysis or using larger size class");
        }
        
        if !info.migration_history.is_empty() {
            println!("  Migration history:");
            for migration in &info.migration_history {
                println!("    {} -> {} (reason: {:?})",
                        migration.from_class,
                        migration.to_class, 
                        migration.reason);
            }
        }
    }
    
    fn validate_metadata_accuracy() -> MetadataValidationReport {
        let mut report = MetadataValidationReport::new();
        
        for (function_id, stats) in get_all_function_stats() {
            let metadata = MetadataRegistry::lookup_metadata(function_id)
                .expect("Missing metadata");
            
            let accuracy = calculate_size_accuracy(&stats, metadata);
            
            if accuracy < ACCURACY_THRESHOLD {
                report.inaccurate_predictions.push(InaccuratePrediction {
                    function_id,
                    predicted_size: metadata.estimated_stack_size,
                    actual_average: stats.average_stack_usage,
                    sample_count: stats.execution_count,
                    accuracy_ratio: accuracy,
                });
            }
        }
        
        report
    }
}
```

### Performance Profiling Integration

```cpp
// Integration with standard profiling tools
class CoroProfilerIntegration {
    vtune_integration: Option<VTuneIntegration>,
    perf_integration: Option<PerfIntegration>, 
    custom_profiler: Option<CustomProfiler>,
    
    constructed_by: ProfilerIntegrationOps,
}

functional class ProfilerIntegrationOps {
    fn start_profiling_session(integration: &mut CoroProfilerIntegration) {
        // Notify profiler about coroutine metadata
        if let Some(ref mut vtune) = integration.vtune_integration {
            for metadata in COROUTINE_METADATA {
                vtune.register_coroutine_function(
                    metadata.function_id,
                    metadata.estimated_stack_size,
                    metadata.size_class
                );
            }
        }
        
        // Set up custom markers for different allocation paths
        Self::setup_allocation_markers(integration);
    }
    
    fn record_allocation_event(
        integration: &CoroProfilerIntegration,
        function_id: u64,
        allocation_source: AllocationSource,
        allocation_time_ns: u64
    ) {
        // Record in all active profilers
        if let Some(ref vtune) = integration.vtune_integration {
            vtune.record_custom_event("coro_allocation", function_id, allocation_time_ns);
        }
        
        if let Some(ref perf) = integration.perf_integration {
            perf.record_allocation_marker(function_id, allocation_source);
        }
    }
}
```

## Best Practices for Protocol Usage

### Compiler Implementation Guidelines

```cpp
// Recommended compiler analysis depth
struct AnalysisConfiguration {
    max_call_depth: u32,           // 50 levels recommended
    max_recursion_analysis: u32,   // 100 iterations recommended
    enable_cross_module: bool,     // true for whole-program optimization
    conservative_fallback: bool,   // true for production, false for profiling
    
    // Size estimation safety margins
    stack_size_safety_factor: f64, // 1.2 recommended (20% safety margin)
    recursion_safety_factor: f64,  // 2.0 recommended (100% safety margin)
}

// Example of conservative vs aggressive analysis
fn analyze_coroutine_function(func: &Function, config: &AnalysisConfiguration) -> CoroMetadata {
    let base_analysis = perform_static_analysis(func);
    
    let estimated_size = if config.conservative_fallback {
        // Conservative: assume worst case for ambiguous situations
        base_analysis.max_case_size * config.stack_size_safety_factor
    } else {
        // Aggressive: use average case, rely on runtime feedback
        base_analysis.average_case_size * config.stack_size_safety_factor
    };
    
    let size_class = if estimated_size <= 256 && base_analysis.is_definitely_bounded {
        CoroSizeTag::Micro
    } else if estimated_size <= 2048 && base_analysis.bounded_with_high_confidence {
        CoroSizeTag::Small
    } else if estimated_size <= 16384 && base_analysis.probably_bounded {
        CoroSizeTag::Medium
    } else {
        CoroSizeTag::Large
    };
    
    CoroMetadata {
        function_id: func.id(),
        estimated_stack_size: estimated_size as usize,
        size_class,
        is_bounded: base_analysis.is_definitely_bounded,
        migration_likelihood: calculate_migration_likelihood(&base_analysis),
        ..Default::default()
    }
}
```

### Library Implementation Guidelines

```cpp
// Recommended pool configuration
struct RecommendedPoolConfig {
    // Conservative defaults for general-purpose applications
    micro_pool_count: usize,      // 4 pools of 1000 slots each
    micro_pool_size: usize,       // 256KB per pool (1000 * 256B)
    
    small_pool_count: usize,      // 2 pools
    small_pool_capacity: usize,   // 500 slots per pool (1MB per pool)
    
    medium_pool_count: usize,     // 1 pool
    medium_pool_capacity: usize,  // 100 slots (1.6MB pool)
    
    // Individual allocation for Large class
    large_allocation_tracking: bool, // true - track for optimization
}

fn create_production_pool_config() -> RecommendedPoolConfig {
    RecommendedPoolConfig {
        micro_pool_count: determine_micro_pool_count(),
        micro_pool_size: 256 * 1000,  // 1000 micro coroutines per pool
        
        small_pool_count: 2,
        small_pool_capacity: 500,
        
        medium_pool_count: 1,
        medium_pool_capacity: 100,
        
        large_allocation_tracking: true,
    }
}

fn determine_micro_pool_count() -> usize {
    // Base on available memory and expected workload
    let available_memory = get_available_memory();
    let expected_micro_coros = estimate_concurrent_micro_coroutines();
    
    let pools_needed = (expected_micro_coros + 999) / 1000;  // Round up
    let max_pools_by_memory = available_memory / (256 * 1024);  // 256KB per pool
    
    pools_needed.min(max_pools_by_memory).max(1)  // At least 1 pool
}
```

## Channel Analysis Integration

### Channel Usage in Coroutine Analysis

The compiler extends its static analysis to include channel operations and their impact on coroutine behavior:

```cpp
// Compiler analysis of channel-using coroutines
async fn message_processor(
    input_ch: Channel<InputMessage>,
    output_ch: Channel<OutputMessage>
) -> ProcessorResult {
    // Compiler analysis includes:
    // - Local variables: 512 bytes
    // - Channel references: 2 * 8 bytes = 16 bytes  
    // - Suspension state per channel op: 2 * 64 bytes = 128 bytes
    // - Channel buffer involvement: May affect suspension likelihood
    // Total estimated: 656 bytes -> CoroSizeTag::Small
    
    loop {
        let input = co_await input_ch.recv()?;    // Suspension point 1
        let processed = process_message(input);
        co_await output_ch.send(processed)?;      // Suspension point 2
        
        // Compiler tracks potential suspension at each channel operation
    }
}

// Enhanced metadata for channel operations
CoroMetadata {
    function_id: hash_of!("message_processor"),
    estimated_stack_size: 656,
    max_stack_size: Some(1024),
    is_bounded: true,
    
    // Channel-specific analysis
    channel_operation_count: 2,
    suspension_point_count: 2,
    channel_patterns: vec![
        ChannelPattern::RecvLoop(input_ch),
        ChannelPattern::SendEach(output_ch),
    ],
    
    // Suspension likelihood affected by channel usage
    migration_likelihood: 0.4,  // Higher due to potential channel blocking
    
    // Channel-specific optimization hints
    preferred_allocation: AllocationHint::ChannelHeavy,
}
```

### Channel Pattern Recognition

The compiler recognizes common channel usage patterns for optimization:

```cpp
// Pattern recognition examples
enum ChannelPattern {
    // Simple patterns
    SendOnly(ChannelId),              // Only sends, never receives
    RecvOnly(ChannelId),              // Only receives, never sends
    
    // Loop patterns  
    RecvLoop(ChannelId),              // Receive loop (common worker pattern)
    SendBatch(ChannelId, usize),      // Send multiple items in batch
    
    // Multi-channel patterns
    Select(Vec<ChannelId>),           // Select on multiple channels
    Pipeline(ChannelId, ChannelId),   // Input -> process -> output
    Fanout(ChannelId, Vec<ChannelId>), // One input, multiple outputs
    Fanin(Vec<ChannelId>, ChannelId), // Multiple inputs, one output
}

// Compiler generates pattern-specific optimizations
struct ChannelAnalysis {
    function_id: FunctionId,
    patterns: Vec<ChannelPattern>,
    blocking_probability: f32,        // Likelihood of blocking on channels
    channel_affinity: ChannelAffinity, // Preferred coroutine pool type
}

enum ChannelAffinity {
    SendOnly,     // Micro coroutines suitable
    RecvOnly,     // Micro coroutines suitable  
    Bidirectional, // Small coroutines needed
    MultiSelect,  // Medium coroutines for select overhead
    Pipeline,     // Optimized for throughput
}

// Pattern-based size classification
fn classify_with_channels(base_analysis: &StackAnalysis, channels: &ChannelAnalysis) -> CoroSizeTag {
    let base_size = base_analysis.estimated_stack_size;
    
    // Adjust based on channel usage patterns
    let channel_overhead = match channels.channel_affinity {
        ChannelAffinity::SendOnly | ChannelAffinity::RecvOnly => 64,      // Minimal
        ChannelAffinity::Bidirectional => 128,                           // Moderate
        ChannelAffinity::MultiSelect => 256,                             // Select state
        ChannelAffinity::Pipeline => 192,                                // Buffer references
    };
    
    let total_size = base_size + channel_overhead;
    
    match (total_size, channels.blocking_probability) {
        (size, _) if size <= 256 => CoroSizeTag::Micro,
        (size, prob) if size <= 2048 && prob < 0.3 => CoroSizeTag::Small,
        (size, prob) if size <= 16384 && prob < 0.7 => CoroSizeTag::Medium,
        _ => CoroSizeTag::Large,
    }
}
```

### Channel Operation Metadata Generation

```cpp
// Extended metadata for channel operations
#[repr(C)]
struct ChannelCoroMetadata {
    // Base coroutine metadata
    base: CoroMetadata,
    
    // Channel-specific metadata
    channel_operations: *const ChannelOperation,
    channel_operation_count: u32,
    
    // Pattern analysis
    primary_pattern: ChannelPattern,
    blocking_operations: u32,           // Number of potentially blocking ops
    select_complexity: u32,             // Number of channels in largest select
    
    // Performance hints
    channel_affinity: ChannelAffinity,
    preferred_pool: ChannelPoolHint,
    wake_frequency: WakeFrequency,      // Expected wake-ups per second
}

struct ChannelOperation {
    operation_type: ChannelOpType,
    channel_id: ChannelId,
    suspension_likelihood: f32,         // 0.0 to 1.0
    stack_usage: usize,                 // Stack space for this operation
}

enum ChannelOpType {
    Send { is_blocking: bool },
    Receive { is_blocking: bool },
    Select { channel_count: u32 },
    Close,
}

// Automatically generated metadata
#[link_section = ".channel_metadata"]
static CHANNEL_COROUTINE_METADATA: &[ChannelCoroMetadata] = &[
    ChannelCoroMetadata {
        base: CoroMetadata {
            function_id: hash_of!("message_processor"),
            estimated_stack_size: 656,
            size_class: CoroSizeTag::Small,
            // ... other fields
        },
        
        channel_operations: &[
            ChannelOperation {
                operation_type: ChannelOpType::Receive { is_blocking: true },
                channel_id: hash_of!("input_ch"),
                suspension_likelihood: 0.8,     // High chance of blocking
                stack_usage: 64,
            },
            ChannelOperation {
                operation_type: ChannelOpType::Send { is_blocking: false },
                channel_id: hash_of!("output_ch"),
                suspension_likelihood: 0.2,     // Lower chance (buffered?)
                stack_usage: 48,
            },
        ] as *const ChannelOperation,
        channel_operation_count: 2,
        
        primary_pattern: ChannelPattern::Pipeline(
            hash_of!("input_ch"), 
            hash_of!("output_ch")
        ),
        blocking_operations: 1,
        select_complexity: 0,
        
        channel_affinity: ChannelAffinity::Pipeline,
        preferred_pool: ChannelPoolHint::PipelineOptimized,
        wake_frequency: WakeFrequency::High,
    },
];
```

### Channel-Aware Pool Selection

The library uses channel analysis for specialized pool selection:

```cpp
// Channel-aware coroutine allocation
functional class ChannelCoroAllocator {
    fn allocate_for_channel_pattern<F>(
        pattern: ChannelPattern,
        metadata: &ChannelCoroMetadata
    ) -> CoroHandle<F::Output>
    where F: CoroFunction {
        let allocation_strategy = select_channel_strategy(pattern, metadata);
        
        match allocation_strategy {
            ChannelStrategy::SendOnlyMicro => {
                // Micro pool optimized for send-only patterns
                allocate_from_send_micro_pool(metadata)
            },
            ChannelStrategy::RecvOnlyMicro => {
                // Micro pool optimized for receive-only patterns  
                allocate_from_recv_micro_pool(metadata)
            },
            ChannelStrategy::PipelineSmall => {
                // Small pool with pipeline optimizations
                allocate_from_pipeline_pool(metadata)
            },
            ChannelStrategy::SelectMedium => {
                // Medium pool with select state space
                allocate_from_select_pool(metadata)
            },
            ChannelStrategy::HighThroughput => {
                // Specialized high-throughput allocation
                allocate_from_throughput_pool(metadata)
            },
        }
    }
    
    fn select_channel_strategy(
        pattern: ChannelPattern,
        metadata: &ChannelCoroMetadata
    ) -> ChannelStrategy {
        match pattern {
            ChannelPattern::SendOnly(_) if metadata.base.estimated_stack_size <= 256 => {
                ChannelStrategy::SendOnlyMicro
            },
            ChannelPattern::RecvOnly(_) if metadata.base.estimated_stack_size <= 256 => {
                ChannelStrategy::RecvOnlyMicro
            },
            ChannelPattern::Pipeline(_, _) => {
                ChannelStrategy::PipelineSmall
            },
            ChannelPattern::Select(channels) if channels.len() > 2 => {
                ChannelStrategy::SelectMedium
            },
            _ if metadata.wake_frequency == WakeFrequency::High => {
                ChannelStrategy::HighThroughput
            },
            _ => ChannelStrategy::General,
        }
    }
}

enum ChannelStrategy {
    SendOnlyMicro,
    RecvOnlyMicro,
    PipelineSmall,
    SelectMedium,
    HighThroughput,
    General,
}
```

### Channel Usage Profiling

Runtime channel usage profiling feeds back to the compiler:

```cpp
// Runtime profiling of channel operations
class ChannelProfiler {
    operation_stats: HashMap<FunctionId, ChannelOpStats>,
    pattern_accuracy: HashMap<FunctionId, PatternAccuracy>,
    wake_patterns: HashMap<FunctionId, WakePattern>,
    
    constructed_by: ChannelProfilerOps,
}

struct ChannelOpStats {
    function_id: FunctionId,
    total_operations: u64,
    
    // Per-operation stats
    send_count: u64,
    recv_count: u64,
    select_count: u64,
    
    // Blocking statistics
    send_blocks: u64,           // How often sends blocked
    recv_blocks: u64,           // How often receives blocked
    select_blocks: u64,         // How often selects blocked
    
    // Timing
    avg_send_latency_ns: f64,
    avg_recv_latency_ns: f64,
    avg_select_latency_ns: f64,
}

struct PatternAccuracy {
    predicted_pattern: ChannelPattern,
    actual_pattern: ChannelPattern,
    accuracy_score: f32,        // 0.0 to 1.0
    misprediction_cost: u64,    // Nanoseconds lost to wrong predictions
}

functional class ChannelProfilerOps {
    fn record_channel_operation(
        profiler: &mut ChannelProfiler,
        function_id: FunctionId,
        operation: ChannelOpType,
        latency_ns: u64,
        did_block: bool
    ) {
        let stats = profiler.operation_stats
            .entry(function_id)
            .or_insert_with(|| ChannelOpStats::new(function_id));
        
        stats.total_operations += 1;
        
        match operation {
            ChannelOpType::Send { .. } => {
                stats.send_count += 1;
                if did_block {
                    stats.send_blocks += 1;
                }
                stats.avg_send_latency_ns = update_average(
                    stats.avg_send_latency_ns,
                    latency_ns as f64,
                    stats.send_count
                );
            },
            ChannelOpType::Receive { .. } => {
                stats.recv_count += 1;
                if did_block {
                    stats.recv_blocks += 1;
                }
                stats.avg_recv_latency_ns = update_average(
                    stats.avg_recv_latency_ns,
                    latency_ns as f64,
                    stats.recv_count
                );
            },
            ChannelOpType::Select { .. } => {
                stats.select_count += 1;
                if did_block {
                    stats.select_blocks += 1;
                }
                stats.avg_select_latency_ns = update_average(
                    stats.avg_select_latency_ns,
                    latency_ns as f64,
                    stats.select_count
                );
            },
            _ => {}
        }
    }
    
    fn analyze_channel_patterns(profiler: &ChannelProfiler) -> ChannelFeedback {
        let mut feedback = ChannelFeedback::new();
        
        for (function_id, stats) in &profiler.operation_stats {
            // Analyze blocking patterns
            let send_block_rate = stats.send_blocks as f64 / stats.send_count.max(1) as f64;
            let recv_block_rate = stats.recv_blocks as f64 / stats.recv_count.max(1) as f64;
            
            // Detect pattern mismatches
            if send_block_rate > 0.5 && recv_block_rate < 0.1 {
                // Mostly blocked on sends, rarely on receives
                feedback.suggest_pattern_change(*function_id, ChannelPattern::SendHeavy);
            } else if recv_block_rate > 0.5 && send_block_rate < 0.1 {
                // Mostly blocked on receives, rarely on sends
                feedback.suggest_pattern_change(*function_id, ChannelPattern::RecvHeavy);
            }
            
            // Suggest size class changes based on blocking frequency
            let total_block_rate = (stats.send_blocks + stats.recv_blocks) as f64 / 
                                 stats.total_operations as f64;
            
            if total_block_rate > 0.8 {
                // High blocking rate - suggest larger coroutines for better batching
                feedback.suggest_size_increase(*function_id, "high blocking rate");
            } else if total_block_rate < 0.1 {
                // Low blocking rate - could use smaller coroutines
                feedback.suggest_size_decrease(*function_id, "low blocking rate");
            }
        }
        
        feedback
    }
}

// Feedback structure for compiler
struct ChannelFeedback {
    pattern_suggestions: Vec<PatternSuggestion>,
    size_suggestions: Vec<SizeSuggestion>,
    pool_optimizations: Vec<PoolOptimization>,
}

struct PatternSuggestion {
    function_id: FunctionId,
    current_pattern: ChannelPattern,
    suggested_pattern: ChannelPattern,
    confidence: f32,
    performance_gain_estimate: f32,
}
```

### Channel-Specific Debugging Support

Enhanced debugging information for channel operations:

```cpp
// Debug information for channel operations
struct ChannelDebugInfo {
    coroutine_id: CoroId,
    function_name: &'static str,
    
    // Channel usage
    active_channels: Vec<ChannelId>,
    suspended_on_channel: Option<ChannelId>,
    suspension_reason: SuspensionReason,
    
    // Performance data
    channel_operations_count: u64,
    total_suspension_time_us: u64,
    avg_wake_latency_ns: f64,
    
    // Pattern analysis
    detected_pattern: ChannelPattern,
    pattern_efficiency: f32,    // 0.0 to 1.0
}

enum SuspensionReason {
    SendBlocked { channel_id: ChannelId, queue_length: usize },
    RecvBlocked { channel_id: ChannelId, queue_empty: bool },
    SelectBlocked { channels: Vec<ChannelId>, timeout: Option<Duration> },
    NotSuspended,
}

functional class ChannelDebugger {
    fn print_channel_coroutine_info(coro_id: CoroId) {
        let info = get_channel_debug_info(coro_id);
        
        println("Channel Coroutine Debug Info:");
        println("  Function: {}", info.function_name);
        println("  Active channels: {:?}", info.active_channels);
        
        match info.suspended_on_channel {
            Some(ch_id) => {
                println("  Currently suspended on channel: {:?}", ch_id);
                println("  Suspension reason: {:?}", info.suspension_reason);
            },
            None => {
                println("  Not currently suspended on channels");
            }
        }
        
        println("  Channel operations: {}", info.channel_operations_count);
        println("  Total suspension time: {} μs", info.total_suspension_time_us);
        println("  Average wake latency: {:.2} ns", info.avg_wake_latency_ns);
        
        println("  Detected pattern: {:?}", info.detected_pattern);
        println("  Pattern efficiency: {:.1}%", info.pattern_efficiency * 100.0);
        
        if info.pattern_efficiency < 0.7 {
            println("  ⚠️  Low pattern efficiency - consider optimization");
        }
    }
    
    fn validate_channel_metadata() -> ChannelValidationReport {
        let mut report = ChannelValidationReport::new();
        
        for metadata in CHANNEL_COROUTINE_METADATA {
            let runtime_stats = get_runtime_channel_stats(metadata.base.function_id);
            
            // Validate blocking predictions
            let predicted_blocking = metadata.blocking_operations as f64 / 
                                   metadata.channel_operation_count as f64;
            let actual_blocking = runtime_stats.blocking_rate;
            
            if (predicted_blocking - actual_blocking).abs() > 0.3 {
                report.blocking_mismatches.push(BlockingMismatch {
                    function_id: metadata.base.function_id,
                    predicted: predicted_blocking,
                    actual: actual_blocking,
                    impact: calculate_performance_impact(predicted_blocking, actual_blocking),
                });
            }
            
            // Validate pattern recognition
            if runtime_stats.actual_pattern != metadata.primary_pattern {
                report.pattern_mismatches.push(PatternMismatch {
                    function_id: metadata.base.function_id,
                    predicted: metadata.primary_pattern,
                    actual: runtime_stats.actual_pattern,
                });
            }
        }
        
        report
    }
}
```

The CPrime compiler-library protocol represents a significant innovation in cooperative multitasking systems, enabling unprecedented cooperation between static analysis and runtime optimization to achieve both safety and performance. The channel analysis extension provides deep insights into communication patterns, enabling optimal coroutine allocation and specialized pool strategies for high-performance concurrent applications.