#include "layer1_debug.h"
#include "token_serializer.h"
#include "chunk_serializer.h"
#include "enum_stringifier.h"
#include <iostream>
#include <fstream>
#include <filesystem>

namespace cprime::layer1validation {

// ============================================================================
// Tokenization Analysis
// ============================================================================

void Layer1Debug::show_tokenization_steps(std::stringstream& stream, StringTable& string_table, bool verbose) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("=== Layer 1 Step-by-Step Tokenization ===");
    LOG_INFO("Input content: {} characters", stream.str().length());
    
    // Capture all intermediate states
    TokenizationStates states = capture_tokenization_states(stream, string_table);
    
    // Display each stage
    display_processing_chunks(states.after_1a, string_table, "Layer 1A: Unambiguous Tokens", verbose);
    display_processing_chunks(states.after_1b, string_table, "Layer 1B: String/Char Literals", verbose);
    display_processing_chunks(states.after_1c, string_table, "Layer 1C: Operators", verbose);
    display_processing_chunks(states.after_1d, string_table, "Layer 1D: Number Literals", verbose);
    display_raw_tokens(states.final_tokens, string_table, "Layer 1E: Final Tokens", true);
    
    LOG_INFO("String table contains {} unique strings", string_table.size());
}

void Layer1Debug::show_final_tokens(std::stringstream& stream, StringTable& string_table, bool show_positions) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("=== Layer 1 Final Tokenization Result ===");
    
    // Just run the tokenizer and show final result
    auto tokens = cprime::Tokenizer::tokenize_stream(stream, string_table);
    
    display_raw_tokens(tokens, string_table, "Final Tokens", show_positions);
    
    LOG_INFO("Total tokens: {}", tokens.size());
    LOG_INFO("String table: {} unique strings", string_table.size());
}

void Layer1Debug::show_sublayer_processing(std::stringstream& stream, StringTable& string_table, const std::string& sublayer_name) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    if (!is_valid_sublayer_name(sublayer_name)) {
        LOG_ERROR("Invalid sublayer name: {}. Valid names: 1a, 1b, 1c, 1d, 1e", sublayer_name);
        return;
    }
    
    LOG_INFO("=== Layer 1 Sublayer {} Analysis ===", get_sublayer_display_name(sublayer_name));
    
    TokenizationStates states = capture_tokenization_states(stream, string_table);
    
    if (sublayer_name == "1a") {
        display_processing_chunks(states.after_1a, string_table, "After Layer 1A", true);
    } else if (sublayer_name == "1b") {
        display_processing_chunks(states.after_1b, string_table, "After Layer 1B", true);
    } else if (sublayer_name == "1c") {
        display_processing_chunks(states.after_1c, string_table, "After Layer 1C", true);
    } else if (sublayer_name == "1d") {
        display_processing_chunks(states.after_1d, string_table, "After Layer 1D", true);
    } else if (sublayer_name == "1e") {
        display_raw_tokens(states.final_tokens, string_table, "After Layer 1E", true);
    }
}

// ============================================================================
// Token Export and Test Creation
// ============================================================================

void Layer1Debug::export_tokens_to_file(std::stringstream& stream, StringTable& string_table, const std::string& output_file) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("Exporting tokens to file: {}", output_file);
    
    // Tokenize the stream
    auto tokens = cprime::Tokenizer::tokenize_stream(stream, string_table);
    
    // Serialize tokens to file
    std::string serialized = TokenSerializer::serialize_tokens(tokens, string_table);
    
    std::ofstream file(output_file);
    if (!file.is_open()) {
        LOG_ERROR("Failed to open output file: {}", output_file);
        return;
    }
    
    file << "# Expected Layer 1 tokenization output" << std::endl;
    file << "# Generated by Layer1Debug::export_tokens_to_file" << std::endl;
    file << "# Input: " << stream.str().length() << " characters" << std::endl;
    file << "# Tokens: " << tokens.size() << std::endl;
    file << std::endl;
    file << serialized << std::endl;
    
    file.close();
    
    LOG_INFO("Successfully exported {} tokens to {}", tokens.size(), output_file);
}

void Layer1Debug::generate_test_case(const std::string& input_content, const std::string& test_case_name, const std::string& test_cases_dir) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("Generating test case '{}' in directory: {}", test_case_name, test_cases_dir);
    
    // Create test case directory
    std::filesystem::path test_case_path = std::filesystem::path(test_cases_dir) / test_case_name;
    std::filesystem::create_directories(test_case_path);
    
    // Write layer1 file (input)
    std::filesystem::path layer1_file = test_case_path / "layer1";
    std::ofstream layer1_stream(layer1_file);
    layer1_stream << input_content;
    layer1_stream.close();
    
    // Generate layer2 file (expected output)
    std::stringstream input_stream(input_content);
    StringTable string_table;
    std::filesystem::path layer2_file = test_case_path / "layer2";
    export_tokens_to_file(input_stream, string_table, layer2_file.string());
    
    LOG_INFO("Test case '{}' created successfully", test_case_name);
    LOG_INFO("  Input file: {}", layer1_file.string());
    LOG_INFO("  Expected output: {}", layer2_file.string());
}

// ============================================================================
// Statistics and Analysis
// ============================================================================

void Layer1Debug::show_tokenization_statistics(std::stringstream& stream, StringTable& string_table) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("=== Layer 1 Tokenization Statistics ===");
    
    auto tokens = cprime::Tokenizer::tokenize_stream(stream, string_table);
    auto token_counts = count_tokens_by_type(tokens);
    
    LOG_INFO("Input: {} characters", stream.str().length());
    LOG_INFO("Total tokens: {}", tokens.size());
    LOG_INFO("");
    LOG_INFO("Token breakdown by raw type:");
    
    for (const auto& [raw_type, count] : token_counts) {
        LOG_INFO("  {}: {}", EnumStringifier::erawtoken_to_string(raw_type), count);
    }
    
    LOG_INFO("");
    auto stats = string_table.get_statistics();
    LOG_INFO("String table statistics:");
    LOG_INFO("  Unique strings: {}", stats.unique_strings);
    LOG_INFO("  Total characters: {}", stats.total_characters);
    LOG_INFO("  Average length: {:.1f}", stats.average_string_length);
    LOG_INFO("  Largest string: {}", stats.largest_string_length);
}

bool Layer1Debug::validate_tokenization(std::stringstream& stream, StringTable& string_table, const std::string& expected_output_file) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("Validating tokenization against: {}", expected_output_file);
    
    // Load expected output
    std::ifstream file(expected_output_file);
    if (!file.is_open()) {
        LOG_ERROR("Failed to open expected output file: {}", expected_output_file);
        return false;
    }
    
    std::string expected_content((std::istreambuf_iterator<char>(file)), std::istreambuf_iterator<char>());
    file.close();
    
    // Tokenize input
    auto actual_tokens = cprime::Tokenizer::tokenize_stream(stream, string_table);
    
    // Parse expected tokens
    StringTable expected_string_table;
    auto expected_tokens = TokenSerializer::parse_expected_output(expected_content, expected_string_table);
    
    // Compare
    std::string diff = TokenSerializer::compare_tokens(expected_tokens, actual_tokens, string_table);
    
    if (diff.empty()) {
        LOG_INFO("✓ Tokenization matches expected output");
        return true;
    } else {
        LOG_ERROR("✗ Tokenization differs from expected output:");
        LOG_ERROR("{}", diff);
        return false;
    }
}

// ============================================================================
// Internal Helper Functions
// ============================================================================

Layer1Debug::TokenizationStates Layer1Debug::capture_tokenization_states(std::stringstream& stream, StringTable& string_table) {
    TokenizationStates states;
    
    // This is a simplified implementation - in reality, you'd need to call each sublayer individually
    // For now, we'll just run the full tokenizer and return the final result
    states.final_tokens = cprime::Tokenizer::tokenize_stream(stream, string_table);
    
    // TODO: Implement individual sublayer calls to capture intermediate states
    // This would require exposing the private sublayer functions or creating a debug-friendly interface
    
    return states;
}

void Layer1Debug::display_processing_chunks(const std::vector<ProcessingChunk>& chunks, 
                                          const StringTable& string_table,
                                          const std::string& stage_name,
                                          bool verbose) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("--- {} ({} chunks) ---", stage_name, chunks.size());
    
    if (verbose) {
        for (size_t i = 0; i < chunks.size(); ++i) {
            LOG_INFO("  [{}] {}", i, ChunkSerializer::serialize(chunks[i], string_table));
        }
    } else {
        LOG_INFO("  {} ProcessingChunk objects (use --verbose for details)", chunks.size());
    }
}

void Layer1Debug::display_raw_tokens(const std::vector<RawToken>& tokens,
                                   const StringTable& string_table,
                                   const std::string& stage_name,
                                   bool show_positions) {
    auto logger = cprime::LoggerFactory::get_logger("layer1_debug");
    
    LOG_INFO("--- {} ({} tokens) ---", stage_name, tokens.size());
    
    for (size_t i = 0; i < tokens.size(); ++i) {
        if (show_positions) {
            LOG_INFO("  [{}] {}", i, TokenSerializer::serialize(tokens[i], string_table));
        } else {
            LOG_INFO("  [{}] {} {}", i, 
                    EnumStringifier::erawtoken_to_string(tokens[i]._raw_token),
                    EnumStringifier::etoken_to_string(tokens[i]._token));
        }
    }
}

std::map<ERawToken, size_t> Layer1Debug::count_tokens_by_type(const std::vector<RawToken>& tokens) {
    std::map<ERawToken, size_t> counts;
    
    for (const auto& token : tokens) {
        counts[token._raw_token]++;
    }
    
    return counts;
}

std::string Layer1Debug::get_sublayer_display_name(const std::string& sublayer_code) {
    if (sublayer_code == "1a") return "Unambiguous Tokens";
    if (sublayer_code == "1b") return "String/Character Literals";
    if (sublayer_code == "1c") return "Operators";
    if (sublayer_code == "1d") return "Number Literals";
    if (sublayer_code == "1e") return "Keywords and Identifiers";
    return "Unknown";
}

bool Layer1Debug::is_valid_sublayer_name(const std::string& sublayer_name) {
    return sublayer_name == "1a" || sublayer_name == "1b" || 
           sublayer_name == "1c" || sublayer_name == "1d" || sublayer_name == "1e";
}

} // namespace cprime::layer1validation